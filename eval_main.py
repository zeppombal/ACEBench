import sys

sys.path.append("../")

import argparse

from category import ACE_DATA_CATEGORY
from model_eval.checker import *
from model_eval.evaluation_helper import *
from model_eval.utils import *
from model_inference.utils import decode_ast

RESULT_TABLE = {}


def normal_single_turn_eval(
    model_result, prompt, possible_answer, test_category, model_name, paths
):
    if not all(len(x) == len(model_result) for x in [prompt, possible_answer]):
        raise ValueError(
            f"The length of the model result ({len(model_result)}) does not match "
            f"the length of the prompt ({len(prompt)}) or possible answer ({len(possible_answer)}). "
            "Please check the input files for completeness."
        )

    result = []
    correct_count = 0
    for i in range(len(model_result)):
        id = prompt[i]["id"]
        question = prompt[i]["question"]
        model_result_item = model_result[i]["result"]
        prompt_item = prompt[i]["function"]
        possible_answer_item = possible_answer[i]["ground_truth"]

        try:
            model_result_item_raw = model_result_item
            model_result_item_raw = "".join(model_result_item_raw.split())
            model_result_item = decode_ast(model_name, model_result_item_raw)
        except Exception as e:
            result.append(
                {
                    "id": id,
                    "valid": False,
                    "error": [f"Invalid syntax. Failed to decode AST. {str(e)}"],
                    "error_type": "wrong_output_format",
                    "model_result_raw": model_result_item_raw,
                    "possible_answer": possible_answer_item,
                }
            )
            continue

        # Check if the output format meets the requirements
        decoder_output_valid = is_function_call_format_valid(model_result_item)
        if not decoder_output_valid:
            result.append(
                {
                    "id": id,
                    "valid": False,
                    "error": [
                        "The output format does not meet the specified requirements."
                    ],
                    "error_type": "wrong_output_format",
                    "model_result": str(model_result_item_raw),
                    "possible_answer": possible_answer_item,
                }
            )
            continue

        if type(possible_answer_item) != list:
            possible_answer_item = [possible_answer_item]

        all_errors = []

        # Filter from multiple candidate answers
        for possible_answer_item_ in possible_answer_item:
            checker_result = normal_checker(
                prompt_item,
                model_result_item,
                possible_answer_item_,
                question,
                test_category,
            )

            if checker_result["valid"]:
                correct_count += 1
                break
            else:
                all_errors.append(
                    {
                        "error": checker_result["error"],
                        "error_type": checker_result["error_type"],
                    }
                )

        if all_errors:
            temp = {
                "id": id,
                "valid": False,
                "error": all_errors[0]["error"],
                "error_type": all_errors[0]["error_type"],
                "model_result": model_result_item_raw,
                "possible_answer": possible_answer_item_,
            }
            result.append(temp)

    accuracy = round((correct_count / len(model_result)), 3)
    result.insert(
        0,
        {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )

    output_file_name = "data_" + test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    save_score_as_json(output_file_name, result, output_file_dir)

    convert_result_to_excel(model_name, test_category, paths)
    return accuracy


def normal_multi_turn_eval(
    model_result, prompt, possible_answer, test_category, model_name, paths
):
    if not all(len(x) == len(model_result) for x in [prompt, possible_answer]):
        raise ValueError(
            f"The length of the model result ({len(model_result)}) does not match "
            f"the length of the prompt ({len(prompt)}) or possible answer ({len(possible_answer)}). "
            "Please check the input files for completeness."
        )

    result = []
    correct_count = 0

    process_score_list = []
    score_list = []

    for i in range(len(model_result)):
        id = model_result[i]["id"]
        turn = prompt[i]["id"].split("_")[-2]
        item = model_result[i]["id"].split("_")[-1]
        question = prompt[i]["question"]
        model_result_item = model_result[i]["result"]
        prompt_item = prompt[i]["function"]
        possible_answer_item_ = possible_answer[i]["ground_truth"]

        try:
            model_result_item_raw = model_result_item
            model_result_item_raw = "".join(model_result_item_raw.split())
            model_result_item = decode_ast(model_name, model_result_item_raw)
        except Exception as e:
            result.append(
                {
                    "id": id,
                    "turn": turn,
                    "valid": False,
                    "error": [f"Invalid syntax. Failed to decode AST. {str(e)}"],
                    "error_type": "wrong_output_format",
                    "model_result": model_result_item_raw,
                    "possible_answer": possible_answer_item_,
                    "process": False,
                    "process_score": 0,
                }
            )

            process_score_list.append(0)

            if len(score_list) > 0 and turn == score_list[-1]["turn"]:
                score_list[-1]["valid"].append(False)
                score_list[-1]["number"] = item
            else:
                score_list.append({"turn": turn, "number": item, "valid": [False]})
            continue
        # Check if the output format meets the requirements
        decoder_output_valid = is_function_call_format_valid(model_result_item)

        if not decoder_output_valid:
            result.append(
                {
                    "id": id,
                    "turn": turn,
                    "valid": False,
                    "error": [
                        "The output format does not meet the specified requirements."
                    ],
                    "error_type": "wrong_output_format",
                    "model_result": str(model_result_item),
                    "possible_answer": possible_answer_item_,
                    "process": False,
                    "process_score": 0,
                }
            )
            process_score_list.append(0)
            if len(score_list) > 0 and turn == score_list[-1]["turn"]:
                score_list[-1]["valid"].append(False)
                score_list[-1]["number"] = item
            else:
                score_list.append({"turn": turn, "number": item, "valid": [False]})
            continue

        if type(possible_answer_item_) != list:
            possible_answer_item_ = [possible_answer_item_]

        all_errors = []

        # Filter from multiple candidate answers
        for possible_answer_item in possible_answer_item_:
            checker_result = normal_checker(
                prompt_item,
                model_result_item,
                possible_answer_item,
                question,
                test_category,
            )

            if checker_result["valid"]:
                correct_count += 1
                process_score_list.append(1)
                break
            else:
                all_errors.append(
                    {
                        "error": checker_result["error"],
                        "error_type": checker_result["error_type"],
                    }
                )

        if not checker_result["valid"]:
            temp = {
                "id": id,
                "turn": turn,
                "valid": False,
                "error": all_errors[0]["error"],
                "error_type": all_errors[0]["error_type"],
                "model_result": model_result_item_raw,
                "possible_answer": possible_answer_item_,
            }
            result.append(temp)

        turn = model_result[i]["id"].split("_")[-2]
        item = model_result[i]["id"].split("_")[-1]
        if len(score_list) > 0 and turn == score_list[-1]["turn"]:
            score_list[-1]["valid"].append(checker_result["valid"])
            score_list[-1]["number"] = item
        else:
            score_list.append(
                {"turn": turn, "number": item, "valid": [checker_result["valid"]]}
            )

    if len(score_list) == 0:
        end_accuracy, process_accuracy = 0, 0
    else:
        end_accuracy, process_accuracy = multiplt_turn_accuracy(score_list)

    result.insert(
        0,
        {
            "accuracy": end_accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
            "process_accuracy": process_accuracy,
        },
    )

    output_file_name = "data_" + test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    save_score_as_json(output_file_name, result, output_file_dir)

    convert_result_to_excel(model_name, test_category, paths)

    return end_accuracy


def special_eval(model_result, prompt, possible_answer, category, model_name, paths):

    if not all(len(x) == len(model_result) for x in [prompt, possible_answer]):
        raise ValueError(
            f"The length of the model result ({len(model_result)}) does not match "
            f"the length of the prompt ({len(prompt)}) or possible answer ({len(possible_answer)}). "
            "Please check the input files for completeness."
        )

    result = []
    wrong_list = []
    correct_count = 0
    for i in range(len(model_result)):

        id = prompt[i]["id"]
        model_result_item = model_result[i]["result"]
        possible_answer_item_ = possible_answer[i]["ground_truth"]
        result.append(
            {
                "id": id,
                "valid": True,
                "error": [],
                "error_type": "",
                "model_result_decoded": str(model_result_item),
                "possible_answer": possible_answer_item_,
            }
        )
        if "incomplete" in category:
            for name, values in possible_answer_item_.items():
                if "Missing necessary parameters" not in model_result_item:
                    result[i]["valid"] = False
                    result[i]["error"] = [
                        f"The user's instruction is missing necessary parameters ({values}) for the ({name}), but the model failed to correctly point it out"
                    ]
                    result[i]["error_type"] = "error_detection"
                elif name not in model_result_item:
                    result[i]["valid"] = False
                    result[i]["error"] = [
                        f"The user's instruction is missing necessary parameters ({values}) for the ({name}), but the model failed to correctly point it out"
                    ]
                    result[i]["error_type"] = "error_correction"
                else:
                    for value in values:
                        if value not in model_result_item:
                            result[i]["valid"] = False
                            result[i]["error"] = [
                                f"The user's instruction is missing necessary parameters ({value}) for the ({name}), but the model failed to correctly point it out"
                            ]
                            result[i]["error_type"] = "error_correction"

        elif "error" in category:
            for name, values in possible_answer_item_.items():
                if "There is incorrect value" not in model_result_item:
                    result[i]["valid"] = False
                    result[i]["error"] = [
                        f"The user's instruction contains incorrect values ({values}) of the parameters ({name}), but the model failed to correctly point it out"
                    ]
                    result[i]["error_type"] = "error_detection"
                else:
                    for value in values:
                        if value not in model_result_item:
                            result[i]["valid"] = False
                            result[i]["error"] = [
                                f"The user's instruction contains incorrect values ({values}) of the parameters ({name}), but the model failed to correctly point it out"
                            ]
                            result[i]["error_type"] = "error_correction"
        elif "irrelevant" in category:
            if "the limitations of the function" not in model_result_item:
                result[i]["valid"] = False
                result[i]["error"] = [
                    f"The model cannot solve this problem, due to the limitations of the function"
                ]
                result[i]["error_type"] = "error_detection"

        if result[i]["valid"]:
            correct_count += 1

    for item in result:
        if item["valid"] == False:
            wrong_list.append(item)

    accuracy = correct_count / len(model_result)
    wrong_list.insert(
        0,
        {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )
    output_file_name = "data_" + category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    save_score_as_json(output_file_name, wrong_list, output_file_dir)
    convert_result_to_excel(model_name, category, paths)
    return accuracy


def agent_eval(model_result, prompt, possible_answer, test_category, model_name):

    if not all(len(x) == len(model_result) for x in [prompt, possible_answer]):
        raise ValueError(
            f"The length of the model result ({len(model_result)}) does not match "
            f"the length of the prompt ({len(prompt)}) or possible answer ({len(possible_answer)}). "
            "Please check the input files for completeness."
        )

    result = []
    correct_index = []
    correct_count = 0

    for i in range(len(model_result)):
        model_result_item = model_result[i]["result"]
        possible_answer_item_ = possible_answer[i]["ground_truth"]

        if type(possible_answer_item_) != list:
            possible_answer_item_ = [possible_answer_item_]

        result_tmp = {
            "id": i,
            "valid": True,
            "error": [],
            "error_type": "",
        }

        is_valid = True
        checker_result = {}
        checker_result["valid"] = True

        if len(possible_answer_item_) != len(model_result_item):
            result_tmp["valid"] = False
            result_tmp["error_type"] = "wrong number of class"
            is_valid = False
        else:
            # Compare each category one by one
            for index in range(len(possible_answer_item_)):
                possible_keys = set(possible_answer_item_[index].keys())
                matched_dict = None
                for model_dict in model_result_item:
                    model_keys = set(model_dict.keys())
                    if possible_keys == model_keys:
                        matched_dict = model_dict
                        break

                if matched_dict:
                    checker_result = agent_checker(
                        matched_dict,
                        possible_answer_item_[index],
                    )

                if checker_result["valid"] == False:
                    result_tmp["valid"] = False
                    result_tmp["error"].append(checker_result["error"])
                    result_tmp["error_type"] = checker_result["error_type"]
                    is_valid = False

        if not is_valid:
            result.append(result_tmp)
        else:
            correct_count += 1
            correct_index.append(i)

    accuracy = round(correct_count / len(model_result), 3)
    process_accuracy = agent_eval_process(
        model_name,
        model_result,
        possible_answer,
        test_category,
        correct_index,
        language,
    )
    result.insert(
        0,
        {
            "end_to_end_accuracy": accuracy,
            "process_accuracy": process_accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )
    output_file_name = "data_" + test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    save_score_as_json(output_file_name, result, output_file_dir)
    return accuracy, process_accuracy


def agent_eval_process(
    model_name, model_results, possible_answers, test_category, correct_list, language
):
    individual_accuracies = []  # Used to store the accuracy of each data point
    total_accuracy = 0  # Store the total accuracy of all data

    for index in range(len(model_results)):
        if index in correct_list:
            accuracy = 1.00
            total_accuracy += 1.00
            continue
        call_process = possible_answers[index]["mile_stone"]
        model_result = model_results[index]["process"]
        if isinstance(call_process[0], list):
            max_accuracy = -1
            for call_process_item in call_process:
                result_len = len(model_result)
                milestone_len = len(call_process_item)

                result_indices = []
                current_index = 0

                # Iterate through each element in call_process and search sequentially
                for stone in call_process_item:
                    # Start searching from the current index until the corresponding call_process element is found
                    while current_index < result_len:
                        if model_result[current_index].strip() == stone.strip():
                            result_indices.append(current_index)
                            current_index += 1
                            break
                        current_index += 1

                # Calculate call_process accuracy using floating-point division
                if milestone_len == 0:
                    accuracy = 1.00
                else:
                    accuracy = len(result_indices) / milestone_len
                rounded_accuracy = round(accuracy, 3)
                if rounded_accuracy > max_accuracy:
                    max_accuracy = rounded_accuracy
                    name = test_category + "_" + str(index)

            # Save the accuracy of each data point
            if accuracy != 1.00:
                individual_accuracies.append(
                    {
                        name: {
                            "process_accuracy": rounded_accuracy,
                            "model_output": model_result,
                            "call_process": call_process,
                        }
                    }
                )
            # Accumulate total accuracy
            total_accuracy += max_accuracy

        # For a single answer, calculate directly
        else:
            result_len = len(model_result)
            milestone_len = len(call_process)

            result_indices = []
            current_index = 0

            # Iterate through each element in call_process and search sequentially
            for stone in call_process:

                while current_index < result_len:
                    if model_result[current_index].strip() == stone.strip():
                        result_indices.append(current_index)
                        current_index += 1  # Update position and continue searching for the next stone
                        break
                    current_index += 1

            # Calculate call_process accuracy using floating-point division
            if milestone_len == 0:
                accuracy = 1.00
            else:
                accuracy = len(result_indices) / milestone_len
            rounded_accuracy = round(accuracy, 3)

            # Save the accuracy of each data point
            name = test_category + "_" + str(index)
            if accuracy != 1.00:
                individual_accuracies.append(
                    {
                        name: {
                            "process_accuracy": rounded_accuracy,
                            "model_output": model_result,
                            "call_process": call_process,
                        }
                    }
                )

            # Accumulate total accuracy
            total_accuracy += accuracy

    # Calculate the overall accuracy of all entries
    overall_accuracy = total_accuracy / len(model_results)
    overall_accuracy = round(overall_accuracy, 3)  # Keep two decimal places
    if language == "zh":
        file_name = (
            "./score_all/score_zh/"
            + model_name
            + "/data_"
            + test_category
            + "_process.json"
        )
    elif language == "en":
        file_name = (
            "./score_all/score_en/"
            + model_name
            + "/data_"
            + test_category
            + "_process.json"
        )
    # Write individual_accuracies to JSON file line by line
    with open(file_name, "w", encoding="utf-8") as f:
        for entry in individual_accuracies:
            json.dump(entry, f, ensure_ascii=False)
            f.write(
                "\n"
            )  # Write a newline character to make each JSON object occupy a separate line

    # Return the accuracy of each data point and the overall accuracy
    return overall_accuracy


#### Main runner function ####
def runner(model_names, categories, paths):

    for model_name in model_names:
        for category in categories:
            print(f"🔍 Running test: {category}")

            model_result_path = build_result_path(
                INPUT_PATH, model_name, category, "_result.json"
            )
            model_result = load_file(model_result_path)

            prompt_path = build_data_path(PROMPT_PATH, category)
            prompt = load_file(prompt_path)

            possible_answer_path = build_data_path(POSSIBLE_ANSWER_PATH, category)
            possible_answer = load_file(possible_answer_path)

            if "special" in category:
                accuracy = special_eval(
                    model_result,
                    prompt,
                    possible_answer,
                    category,
                    model_name,
                    paths,
                )
                print(
                    f"Model: {model_name} | ✔️ Test '{category}' is done! 🚀 Accuracy: {accuracy}."
                )

            elif "agent" in category:
                end_accuracy, process_accuracy = agent_eval(
                    model_result,
                    prompt,
                    possible_answer,
                    category,
                    model_name,
                )
                print(
                    f"Model: {model_name} | ✔️ Test '{category}' is done! | End_to_End Accuracy: {end_accuracy} | Process Accuracy: {process_accuracy}"
                )

            elif "normal_multi_turn" in category:
                end_accuracy = normal_multi_turn_eval(
                    model_result,
                    prompt,
                    possible_answer,
                    category,
                    model_name,
                    paths,
                )
                print(
                    f"Model: {model_name} | ✔️ Test '{category}' is done! | Accuracy: {end_accuracy}"
                )

            else:
                accuracy = normal_single_turn_eval(
                    model_result,
                    prompt,
                    possible_answer,
                    category,
                    model_name,
                    paths,
                )
                print(
                    f"Model: {model_name} | ✔️ Test '{category}' is done! | Accuracy: {accuracy}"
                )

    update_result_table_with_score_file(RESULT_TABLE, OUTPUT_PATH)
    generate_result_csv(RESULT_TABLE, OUTPUT_PATH)


def get_paths(language):
    base_paths = {
        "zh": {
            "INPUT_PATH": "./result_all/result_zh/",
            "PROMPT_PATH": "./data_all/data_zh/",
            "POSSIBLE_ANSWER_PATH": "./data_all/data_zh/possible_answer/",
            "OUTPUT_PATH": "./score_all/score_zh/",
        },
        "en": {
            "INPUT_PATH": "./result_all/result_en/",
            "PROMPT_PATH": "./data_all/data_en/",
            "POSSIBLE_ANSWER_PATH": "./data_all/data_en/possible_answer/",
            "OUTPUT_PATH": "./score_all/score_en/",
        },
    }
    return base_paths.get(language)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process two lists of strings.")

    parser.add_argument("--language", type=str, default="zh")
    parser.add_argument(
        "--model", nargs="+", type=str, help="A list of model names to evaluate"
    )
    parser.add_argument(
        "--category",
        nargs="+",
        type=str,
        help="A list of test categories to run the evaluation on",
    )

    args = parser.parse_args()

    paths = get_paths(args.language)

    if paths:
        INPUT_PATH = paths["INPUT_PATH"]
        PROMPT_PATH = paths["PROMPT_PATH"]
        POSSIBLE_ANSWER_PATH = paths["POSSIBLE_ANSWER_PATH"]
        OUTPUT_PATH = paths["OUTPUT_PATH"]

    # Extract test categories
    test_categories = [
        category
        for test_category in (args.category or [])
        for category in (ACE_DATA_CATEGORY.get(test_category, [test_category]))
    ]

    # Extract and normalize model names
    model_names = [model_name for model_name in (args.model or [])]

    # Get language
    language = args.language

    # Call the main function
    runner(model_names, test_categories, paths)
    # print the scores we care about in a txt to print to excel
    all_folder = f"{OUTPUT_PATH}/{model_names[0]}"
    with open(f"{all_folder}/agg_results.txt", "w") as f:
        for task_res in [
            "data_agent_multi_step_score.json",
            "data_agent_multi_turn_score.json",
            "data_normal_atom_bool_score.json",
            "data_normal_atom_enum_score.json",
            "data_normal_atom_list_score.json",
            "data_normal_atom_number_score.json",
            "data_normal_atom_object_deep_score.json",
            "data_normal_atom_object_short_score.json",
            "data_normal_multi_turn_user_adjust_score.json",
            "data_normal_multi_turn_user_switch_score.json",
            "data_normal_preference_score.json",
            "data_normal_similar_api_score.json",
            "data_normal_single_turn_parallel_function_score.json",
            "data_normal_single_turn_single_function_score.json",
            "data_special_error_param_score.json",
            "data_special_incomplete_score.json",
            "data_special_irrelevant_score.json",
        ]:
            with open(f"{all_folder}/{task_res}", "r") as f2:
                first_line = f2.readline().strip()
                data_dict = json.loads(first_line)
            if "agent" in task_res:
                f.write(str(data_dict["end_to_end_accuracy"]))
                f.write("\t")
                f.write(str(data_dict["process_accuracy"]))
            else:
                f.write(str(data_dict["accuracy"]))
            f.write("\t")
    print(f"Models being evaluated: {model_names}")
    print(f"Test categories being used: {test_categories}")
